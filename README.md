# Reproduce Pre-training GPT-2 on OpenWebText

## Introduction

This project reproduces the pre-training of GPT-2 on OpenWebText. The project adopts most codebase from the open-source implementation of GPT-2, [nanoGPT](https://github.com/karpathy/nanoGPT), but re-organizes the key arguments using [Pydantic](https://docs.pydantic.dev/latest/), which provides a easy-to-adapt codebase for future experiments.

## Results

## Reproduce Experiments
